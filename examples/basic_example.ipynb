{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samgregson/prompt_optimizer/blob/main/examples/basic_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup for Colab"
      ],
      "metadata": {
        "id": "FvGmAPaQ7n8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "  !pip install git+https://github.com/samgregson/prompt_optimizer.git\n",
        "  !pip install openai\n",
        "  import os\n",
        "  import openai\n",
        "  from google.colab import userdata\n",
        "  os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "  openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "8TUenD5u5N4p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic steps:\n",
        "1. Add a `@llm_node` decorator to an LLM function\n",
        "2. Create an evaluation template\n",
        "3. Define your \"training\" data\n",
        "4. Set up the optimizer\n",
        "5. Check the results!"
      ],
      "metadata": {
        "id": "GQ7PVfP47azs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports:"
      ],
      "metadata": {
        "id": "Z90n6Wab71Km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import dedent\n",
        "from openai import OpenAI\n",
        "from prompt_optimizer import PipelineOptimizer, llm_node, OpenAIAdapter"
      ],
      "metadata": {
        "id": "UGgtn8db751E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. LLM 'Program' to optimise:"
      ],
      "metadata": {
        "id": "sWC38qco78JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()  # ensure you have OPENAI_API_KEY in environmental variables / secrets\n",
        "\n",
        "system_prompt = \"you are a helpful assistent\"\n",
        "\n",
        "# Add a `llm_node` decorator around a function which takes a prompt to be optimised\n",
        "# the input should be a component to be optimised (`system_prompt`` in this case)\n",
        "@llm_node(system_prompt=system_prompt)\n",
        "def answer_query(query: str, system_prompt: str = system_prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "jJxYkFKv8B1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Eval Template:"
      ],
      "metadata": {
        "id": "WHpcS1wX8JUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a template for evaluating the final answer\n",
        "# you may use {pipeline_output} and any other custom key word arguments (kwargs)\n",
        "exact_match_evaluator = dedent(\n",
        "    \"\"\"\n",
        "    Your task is to judge the quality if the response given by an AI assistent.\n",
        "    Below are the <question>, <assistent_response> and <expected_answer>.\n",
        "    A correct answer would be an exact string match between the <assistent_response> and the\n",
        "    <expected_answer>\n",
        "\n",
        "    <question>{query}</question>\n",
        "    <assistent_response>{pipeline_ouput}</assistent_response>\n",
        "    <expected_answer>{golden_answer}</expected_answer>\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "-EMsFD8D8Qv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data:"
      ],
      "metadata": {
        "id": "YUH--U328SDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your dataset, this could have any number of items in the dictionary\n",
        "# and contain any number of examples\n",
        "data = [{\n",
        "    \"query\": \"what is the capital of France?\",\n",
        "    \"golden_answer\": \"<answer>Paris</answer>\"\n",
        "}]"
      ],
      "metadata": {
        "id": "n6Fb6LTQ8Vwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Optimize:"
      ],
      "metadata": {
        "id": "7JOZVucj8XGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa28k_0b4Oqu",
        "outputId": "86f25da0-2323-4cc0-a704-f4a79594d0bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer_query': {'system_prompt': <prompt_optimizer.prompt_optimizer.OptimizableComponent at 0x7bec5d86cf10>}}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Instantiate an optimizer and pass in an `LLMCallable`\n",
        "# (in this case using the OpenAIAdapter)\n",
        "llm = OpenAIAdapter(client)\n",
        "optimizer = PipelineOptimizer(llm)\n",
        "\n",
        "# Optimize\n",
        "optimizer.optimize(\n",
        "    iterations=1,\n",
        "    pipeline_func=answer_query,\n",
        "    evaluation_template=exact_match_evaluator,\n",
        "    data=data,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Check the results:"
      ],
      "metadata": {
        "id": "99isisUQ8aZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the results\n",
        "result = answer_query(\"what is the capital of England?\").value\n",
        "\n",
        "print(\"#### RESULT: ####\")\n",
        "print(result)\n",
        "# ---- expected answer: ----\n",
        "# <answer>London</answer>\n",
        "\n",
        "print()\n",
        "print(\"#### PROMPT INFO: ####\")\n",
        "print(optimizer.get_prompt_info())\n",
        "# ---- expected answer: ----\n",
        "# Node: `answer_query`\n",
        "#  system_prompt: Provide the answer strictly in the required format, enclosing your response within <answer></answer> tags. Ensure that the content is clear, concise, and directly addresses the question while maintaining a helpful tone."
      ],
      "metadata": {
        "id": "UkTvUCLQ7ODi",
        "outputId": "65c11a2b-de06-4511-f8cf-0d1811ab916d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#### RESULT: ####\n",
            "<answer>London</answer>\n",
            "\n",
            "#### PROMPT INFO: ####\n",
            "Node: `answer_query`\n",
            "  system_prompt: Respond succinctly and directly to questions. Provide answers in the format <answer>your_answer</answer> without additional wording. Ensure that your response strictly adheres to the format and content specified in the question without extraneous information. Always answer questions directly and avoid unnecessary elaboration.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}